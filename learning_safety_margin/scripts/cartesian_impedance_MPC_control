#!/usr/bin/env python3

import time
import sys
import numpy as np
import rospy
import state_representation as sr
from controllers import create_cartesian_controller, CONTROLLER_TYPE
from dynamical_systems import create_cartesian_ds, DYNAMICAL_SYSTEM_TYPE
from network_interfaces.control_type import ControlType
from network_interfaces.zmq.network import CommandMessage
from learning_safety_margin.robot_interface import RobotInterface

import pickle
from learning_safety_margin.cbf_mpc_pos_planner import *
from learning_safety_margin.control_utils import *

def control_loop(planner, robot, freq): #planner,
    target_set = False
    command = CommandMessage()
    command.control_type = [ControlType.EFFORT.value]

    ds = create_cartesian_ds(DYNAMICAL_SYSTEM_TYPE.POINT_ATTRACTOR)
    ds.set_parameter_value("gain", [50., 50., 50., 10., 10., 10.], sr.ParameterType.DOUBLE_ARRAY)

    ctrl = create_cartesian_controller(CONTROLLER_TYPE.VELOCITY_IMPEDANCE)


    ctrl.set_parameter_value("stiffness", 10., sr.ParameterType.DOUBLE) # 100 100 5 5 - good: 80 80 5 5
    ctrl.set_parameter_value("damping", 5., sr.ParameterType.DOUBLE) # 35 35 2. 2.5
    ctrl.set_parameter_value("inertia", .1, sr.ParameterType.DOUBLE)

    rate = rospy.Rate(freq)
    time_period = 1./freq

    alpha = 0.99
    vel_filtered = np.zeros(3)

    planner_count = 0
    planner_freq = 40 # number of steps to follow planned traj before re-computing
    computing_count = 0


    while not rospy.is_shutdown():
        state = robot.get_state()

        if not state:
            continue
        print("EEF position: ", state.ee_state.get_position())
        #print("EEF orientation: ", state.ee_state.get_orientation())

        if not target_set:
            # TODO : get traget from demos ??
            target = sr.CartesianPose(state.ee_state.get_name(), np.array([.5, .0, .5]), np.array([0., 1., 0., 0.]),
                                      state.ee_state.get_reference_frame())
            ds.set_parameter_value("attractor", target, sr.ParameterType.STATE, sr.StateType.CARTESIAN_POSE)
            target_set = True
        else:

            # Plan one trajectory from initial position to end goal
            if computing_count == 0:
                computing_count += 1
                #planner_count += 1
                print("MPC planning trajectory :", computing_count)
                start_time = time.time()
                X, U = planner.control(state.ee_state.get_position(), target.data()[:3])
                print("Finished planning trajectory :", computing_count)
                print("--- %s seconds --- \n" % (time.time() - start_time))

            # get desired twist from MPC planner
            # plan trajectory from predicted end of previous planner to end goal
            # if planner_count % planner_freq == 0:
            #     planner_count = 0
            #     computing_count +=1
            #     start_time = time.time()
            #     print("EEF position: ", state.ee_state.get_position())
            #     #X, U = planner.control(X[:,planner_freq], target.data()[:3])
            #     X, U = planner.control(state.ee_state.get_position(), target.data()[:3])
            #     print("MPC planning trajectory :", computing_count, "\n")
            #     print("--- %s seconds --- \n" % (time.time() - start_time))

            feedback_state = state.ee_state

            # Get twist using MPC planner
            # planner_count +=1
            #


            # print("MPC velocity command : ", U[:,planner_count-1])
            # print("MPC predicted state: ", X[:,planner_count-1])
            # print("EEF position: ", state.ee_state.get_position())

            # Must get command corresponding to closest current state
            # Rather move to next step only when current state is closer
            state_diff = np.linalg.norm(feedback_state.get_position() - X[planner_count,:])
            next_state_diff = np.linalg.norm(feedback_state.get_position() - X[planner_count+1,:])
            print("state diff: ", state_diff)

            if next_state_diff < state_diff: # move to next state
                print("Going to next state ! \n")
                planner_count +=1

            # exit after trying to play entire traj
            if planner_count > len(np.array(U[:, 0])):  
                break

            desired_twist = sr.CartesianTwist(name=state.ee_state.get_name(), linear_velocity=np.array(U[planner_count,:]).transpose(),
                                      reference=state.ee_state.get_reference_frame())

            #planner_count += 1

            # Get twist using DS
            # desired_twist = sr.CartesianTwist(ds.evaluate(state.ee_state))


            # Clamp to avoid super high velocities -avoids overshoot
            print("Twist pose : ", desired_twist.data())
            desired_twist.clamp(.5, .5)
            print("Twist pose after clamp : ", desired_twist.data())

            # Convert twist back to cartesian state for controller
            desired_state = sr.CartesianState(state.ee_state.get_name(), state.ee_state.get_reference_frame())
            desired_state.set_twist(desired_twist.data())

            ## DEBUG PRINTS
            # fix weird orientation init in Cartesian state (1,0,0,0)
            #desired_state.set_orientation(target.get_orientation())

            # print("desired vel before clamp : ", desired_state.get_linear_velocity())
            # desired_state.clamp_state_variable(.25, sr.CartesianStateVariable.LINEAR_VELOCITY)
            # print("desired vel after clamp : ", desired_state.get_linear_velocity())

            # Set desired position
            # pos_d = state.ee_state.get_position() + time_period * desired_state.get_linear_velocity()
            # desired_state.set_position(pos_d)

            # vel_filtered = (1 - alpha) * vel_filtered + alpha * state.ee_state.get_linear_velocity()
            # feedback_state.set_linear_velocity(vel_filtered)

            # state_error = desired_state- state.ee_state
            # print("Feedback state:", state.ee_state)
            # print("Desired state:", desired_state)
            # print("state error:", state_error)
            # print("Current positions:", state.ee_state.get_position())
            # print("desired positions:", desired_state.get_position())
            # print("Current vel:", state.ee_state.get_linear_velocity())
            # print("desired vel:", desired_state.get_linear_velocity())
            #print("Vel error:", state_error.get_linear_velocity())
            #print("CTRL :", ctrl.get_parameters())

            # print("Current acc:", state.ee_state.get_linear_acceleration())
            # print("desired acc: ", desired_state.get_linear_acceleration())


            #command.joint_state = ctrl.compute_command(desired_state, state.ee_state, state.jacobian)

            # why copying only torques? could send entire command no ?
            command_torques = sr.JointTorques(ctrl.compute_command(desired_state, feedback_state, state.jacobian))
            command.joint_state = state.joint_state
            command.joint_state.set_torques(command_torques.get_torques())
            print("timestep :", planner_count)
            print("Command Torques:", command.joint_state.get_torques())
            robot.send_command(command)


        rate.sleep()


if __name__ == '__main__':
    rospy.init_node("test", anonymous=True)

    robot_interface = RobotInterface("*:1701", "*:1702")

    # Check passed argument - User number
    if len(sys.argv) >= 2:
        user_number = sys.argv[1]
    else:
        user_number = 0

    if isinstance(user_number, str):
        print("Processing rosbags for User_" + user_number)

    # Set up data path
    data_dir = "/home/ros/ros_ws/src/learning_safety_margin/data/User_1/"
    # TODO : make user specific ! Figure out hwo to pass argument using launch file

    data = pickle.load(open(data_dir + "data_dict.p", "rb"))
    params = data["theta"]
    bias_param = data["bias"]
    slack_param = data["unsafe_slack"]
    bias_param = 0.1

    # Initialize RBF Parameters
    print(ws_lim, x_lim, y_lim, z_lim, x_dim, n_dim_features, rbf_std)
    centers, stds = rbf_means_stds(X=None, X_lim=np.array([x_lim, y_lim, z_lim]),
                                   n=x_dim, k=n_dim_features, fixed_stds=True, std=rbf_std)

    freq = 100
    dt = 1./freq

    mpc_planner = CBFMPC_Controller(centers, stds, params, bias_param, dt=dt, n_steps=50)

    control_loop(mpc_planner, robot_interface, freq)

    #control_loop( robot_interface, 200)
