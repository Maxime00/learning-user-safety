#!/usr/bin/env python3

import pandas as pd
import numpy as np
import argparse
import pickle
import random
import rospy
import glob
import time
import os
import state_representation as sr
from controllers import create_joint_controller, create_cartesian_controller, CONTROLLER_TYPE
from dynamical_systems import create_cartesian_ds, DYNAMICAL_SYSTEM_TYPE
from network_interfaces.control_type import ControlType
from network_interfaces.zmq.network import CommandMessage
from learning_safety_margin.robot_interface import RobotInterface
from learning_safety_margin.online_plotting import plot_f0_traj, plot_rec_from_replay, plot_rec_from_planned_joint
from sensor_msgs.msg import JointState
from std_msgs.msg import Header

from learning_safety_margin.cbf_mpc_vel_planner import *
from learning_safety_margin.vel_control_utils import *
from learning_safety_margin.cbf_traj_generator import trajGenerator
from robot_model import Model, InverseKinematicsParameters, QPInverseVelocityParameters
from scipy import interpolate
from pyquaternion import Quaternion

def convert_joint_state_msg(state):
    # Convert sr.joint_state msg to ros JointState msg
    header = Header()
    header.stamp = rospy.get_rostime()
    #header.frame_id = state.get_reference_frame() # is this important ??
    names = state.joint_state.get_names()
    pos = state.joint_state.get_positions()
    vel = state.joint_state.get_velocities()
    effort = state.joint_state.get_torques()
    msg = JointState(header, names, pos, vel, effort)
    return msg

def find_closest_time(df, time):
    dist = (df['time'] - time).abs()
    return df.loc[dist.idxmin()]

def desired_orientation():
    # add -45 deg compensation (ref frame offset from link8 to hand)
    offset_rot = Quaternion(axis=[0., 0., 1.], degrees=45)  # 0.9238, 0.0, 0.3826, 0.)
    temp_orient = Quaternion(0., 1., 0., 0.)
    desired_orientation = temp_orient * offset_rot
    return desired_orientation.elements

def setup_jp(cart_pos, joint_state, robot_model):
    # Set up initial position in Joint State from cartesian state and urdf model
    initial_cart_pos = sr.CartesianState("panda_ee", robot_model.get_base_frame())
    # add z-axis offset for gripper of value 0.1034
    if not in_sim :
        cart_pos = cart_pos + [0, 0, 0.1034]
    initial_cart_pos.set_position(cart_pos)
    initial_cart_pos.set_orientation(desired_orientation())
    param = InverseKinematicsParameters()
    param.max_number_of_iterations = 7000
    param.tolerance = 0.005
    initial_joint_pos_sr = sr.JointPositions(
        robot_model.inverse_kinematics(sr.CartesianPose(initial_cart_pos), sr.JointPositions(joint_state), param, "panda_link8"))
    return initial_joint_pos_sr.data()

def control_loop(cart_traj_dict, joint_traj_list, robot, freq, replays_flag, do_plots, in_sim):

    # create publisher
    pub = rospy.Publisher('/joint_states', JointState, queue_size=10)

    command = CommandMessage()
    command.control_type = [ControlType.EFFORT.value]

    # DS for orientation control
    ds = create_cartesian_ds(DYNAMICAL_SYSTEM_TYPE.POINT_ATTRACTOR)
    ds.set_parameter_value("gain", [0., 0., 0., 5., 5., 5.], sr.ParameterType.DOUBLE_ARRAY)

    # SET UP TORQUE controller - reach start position
    nb_joints = 7
    joint_ctrl = create_joint_controller(CONTROLLER_TYPE.IMPEDANCE, nb_joints)

    if in_sim:  # SIMULATOR GAINS
        joint_ctrl.set_parameter_value("stiffness", [10., 5., 4., 2., 2, 2, 2], sr.ParameterType.DOUBLE_ARRAY)
        joint_ctrl.set_parameter_value("damping", [3., 2., 2., 1., .5, .5, .5], sr.ParameterType.DOUBLE_ARRAY)
        stiffness = np.array([10., 5., 4., 2., 2, 2, 2])
        damping = np.array([3., 2., 2., 1., .5, .5, .5])
    else:  # IRL GAINS
        # Getting to start position
        joint_ctrl.set_parameter_value("stiffness", [220, 200, 200, 180, 120, 100, 80], sr.ParameterType.DOUBLE_ARRAY)
        joint_ctrl.set_parameter_value("damping", [15, 15, 15, 13, 9, 8, 6], sr.ParameterType.DOUBLE_ARRAY)
        # Trajectory following
        stiffness = np.array([220, 220, 200, 180, 110, 100, 40]) * 0.3
        damping = np.array([18, 17, 15.5, 13, 6, 5, 1.2]) * 2.0 # 2.3 max lower this to avoid oscillations # reuse alberic gains ?? 6.5, 5.5 maybe ??

    # Import panda urdf
    urdf_path = "/home/ros/ros_ws/src/learning_safety_margin/urdf/panda_arm.urdf"
    robot_model = Model('panda', urdf_path)

    # Set up state variables
    desired_state = sr.CartesianState('panda_ee', robot_model.get_base_frame())
    feedback_state = sr.CartesianState('panda_ee', 'panda_base')
    desired_joint_state = sr.JointState("franka", nb_joints)
    feedback_joint_state = sr.JointState("franka", nb_joints)

    flag = 1
    print_freq = freq*10

    # Avoid obstacle logic
    obstacle_height = 0.15
    cleared_height = obstacle_height + 0.1
    avoiding_obstacle = False

    # Integrator Variables
    if in_sim:
        inte_gain = [3, 2.5, 2, 1.8, 1.5, 1.3, 1]   # SIM GAINS
    else:
        inte_gain = [65, 55, 50, 45, 40, 30, 20]   #IRL GAINS
    start_integrator_threshold = 0.3 # ideal 0.25 but doesn't reach that sometimes
    init_pos_err_threshold = 0.02
    integrator_reset_threshold = 0.05
    time_period = 1. / freq
    integral = np.zeros(nb_joints)
    inte_torques = np.zeros(nb_joints)
    dq_filtered = np.zeros(nb_joints)

    # filter parameters
    n_dim = 3  # cartesian space[x,y,z]]
    alpha_joint = 0.4
    alpha = 0.8
    alpha_ang = 0.8
    dx_filtered = np.zeros(n_dim)
    dq_filtered_ang = np.zeros(n_dim)
    beta = 0.95
    ddq_filtered = np.zeros(n_dim)

    # Set up data recording
    recorded_state = np.zeros((15000, 1 + nb_joints * 6 + n_dim * 6 + 8)) * np.nan
    idx = 0

    js_record = np.zeros((15000, 1 + nb_joints * 5)) * np.nan
    js_idx = 0

    user_classification = []
    real_classification = []
    final_fn_list = []
    ctrl_labels = []

    # wait a bit to receive correct state
    checked_first_state = False
    checked_obstacle = False

    home_position = np.array([0.0, 0.0, 0.0,  -1.6478, 0.0, 1.6295, 0.785])
    target = home_position

    # Successive traj follow - make random idx list to shuffle planned trajectories
    nbr_planned_traj = len(cart_traj_dict['Labels'])
    # Whether to use replays or only planned trajectories
    if replays_flag:
        idx_list = list(range(0, nbr_planned_traj+len(joint_traj_list)))
    elif not replays_flag:
        idx_list = list(range(0, nbr_planned_traj))
    random.shuffle(idx_list)
    idx_to_incr = 0

    ### Set up first trajectory
    traj_idx = idx_list[idx_to_incr]

    # Set up controller flag
    if traj_idx >= nbr_planned_traj:
        ctrl_flag = 'Replay'
    elif traj_idx < nbr_planned_traj:
        ctrl_flag = 'Planned'

    ## Set up initial position + grab data
    if ctrl_flag == 'Planned':
        ## Set up cartesian trajectory + initial position
        full_desired_X = cart_traj_dict['X'][traj_idx]
        full_desired_U = cart_traj_dict['U'][traj_idx]
        full_desired_T = cart_traj_dict['T'][traj_idx]
        correct_label = cart_traj_dict['Labels'][traj_idx]
        # append 0 accelerations to U to match size of other variables
        full_desired_U = np.append(full_desired_U, [[0, 0, 0]], axis=0)
        initial_joint_pos = setup_jp(full_desired_X[0, 0:3], sr.JointPositions(robot_model.get_robot_name(),home_position), robot_model)

    elif ctrl_flag == 'Replay':
        ## Set up joint trajectory + initial position
        traj = pd.read_pickle(fpath_list[traj_idx - nbr_planned_traj])
        initial_joint_pos = traj['position'].at[0]

    print_count = 0
    target = initial_joint_pos

    ### CONTROL LOOP
    rate = rospy.Rate(freq)
    while not rospy.is_shutdown():

        state = robot.get_state()

        if not state:
            continue

        # Wait a bit so state is correct
        elif state and not checked_first_state:
            # print("EEF position: ", state.ee_state.get_position())
            rospy.sleep(0.1)
            checked_first_state = True
            input("Waiting for user input... Press Enter to start")
            print("Going to home position...")
            start_f0 = time.time()
            continue

        if flag == 1:
            #----------- GO TO START POSITION ---------------------

            error = abs(initial_joint_pos - state.joint_state.get_positions())
            time_f0 = time.time() - start_f0

            if np.all(error < init_pos_err_threshold):
                print("Error :", error)
                print("Reached start position in %s seconds" % time_f0)
                flag = 2

                # Stop robot
                command.joint_state = state.joint_state
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)
                rospy.sleep(1.0)
                robot.send_command(command)

                ## plot f0 traj
                js_record = js_record[~np.isnan(js_record).any(axis=1)]  # remove zeros
                if do_plots:
                    plot_f0_traj(js_record)

                ## Setup trajectory following
                if ctrl_flag == 'Planned':
                    # Create interpolation functions
                    f_X = interpolate.interp1d(full_desired_T, full_desired_X, axis=0)
                    f_U = interpolate.interp1d(full_desired_T, full_desired_U, axis=0)

                    # set DS target for orientation control
                    target_ds = sr.CartesianPose(state.ee_state.get_name(),  full_desired_X[-1, 0:3],  desired_orientation(),
                                                state.ee_state.get_reference_frame())  #robot_model.get_base_frame())#
                    ds.set_parameter_value("attractor", target_ds, sr.ParameterType.STATE, sr.StateType.CARTESIAN_POSE)
                    # Set up for end of traj follow
                    max_time = full_desired_T[-1]

                    joint_ctrl.set_parameter_value("stiffness", stiffness.tolist(), sr.ParameterType.DOUBLE_ARRAY)
                    joint_ctrl.set_parameter_value("damping", damping.tolist(), sr.ParameterType.DOUBLE_ARRAY)

                    print("Ready to play planned trajectory labeled #%s of %s [Progress: %s/%s] \n" % (traj_idx + 1, nbr_planned_traj, idx_to_incr+1, len(idx_list)))

                elif ctrl_flag == 'Replay':
                    # Set up for end of traj follow
                    max_time = traj['time'].iat[-1]
                    print("Ready to play replay trajectory labeled #%s of %s [Progress: %s/%s] \n" % (traj_idx-nbr_planned_traj + 1, len(fpath_list), idx_to_incr+1, len(idx_list)))

                input("Waiting for user input... Press Enter to start \n ")
                rospy.sleep(0.2)
                timeZero = time.time()
                continue

            ## Check if too low, set target accordingly
            if not checked_obstacle :
                if state.ee_state.get_position()[2] < cleared_height:
                    cart_target = state.ee_state.get_position()[0:3]
                    cart_target[2] = cleared_height + 0.1
                    target = setup_jp(cart_target, state.joint_state, robot_model)
                    avoiding_obstacle = True
                    print("Going up !")
                checked_obstacle = True

            # once high enough, switch target to home
            elif checked_obstacle :
                if state.ee_state.get_position()[2] >= cleared_height and avoiding_obstacle:
                    avoiding_obstacle = False
                    target = initial_joint_pos
                    print("Reached cleared height !!")

            # Adding integrator when getting close to target
            if np.all(error < start_integrator_threshold):
                integral += (initial_joint_pos - state.joint_state.get_positions()) * time_period
                # reset integral if overhsoot
                if np.any(abs(integral) > integrator_reset_threshold):
                    integral *= 0
                inte_torques = inte_gain * integral

            # Set feedback state
            feedback_joint_state.set_positions(state.joint_state.get_positions())
            dq_filtered = alpha_joint * dq_filtered + (1-alpha_joint) * state.joint_state.get_velocities()
            feedback_joint_state.set_velocities(dq_filtered)

            # Set velocity and position error
            velo_d = target - state.joint_state.get_positions()

            # Norm velocities
            velo_d = velo_d / np.linalg.norm(velo_d) if np.linalg.norm(velo_d) > 1.0 else velo_d
            desired_joint_state.set_velocities(velo_d)

            pos_d = feedback_joint_state.get_positions() + time_period * velo_d
            desired_joint_state.set_positions(pos_d)

            ## TODO : ADD NULL SPACE
            # Calculate jacobian pseudo inverse

            # calculate null space matrix

            # get null space error


            # Set command
            command_torques = sr.JointTorques(joint_ctrl.compute_command(desired_joint_state, feedback_joint_state))
            command.joint_state = state.joint_state
            command.joint_state.set_torques(command_torques.get_torques() + inte_torques)

            # record data for plots
            js_record[js_idx, 0] = time_f0
            js_record[js_idx, 1:8] = state.joint_state.get_positions()
            js_record[js_idx, 8:15] = state.joint_state.get_velocities()
            js_record[js_idx, 15:22] = initial_joint_pos
            js_record[js_idx, 22:29] = integral
            js_record[js_idx, 29:36] = inte_torques
            js_idx += 1

            # debug print
            if print_count % print_freq == 0:
                print("Error :", error)
            print_count += 1

            # print("Command:", command.joint_state.get_torques())
            if np.any(command_torques.get_torques() > 30):
                print("Command:", command_torques.get_torques())
                print("TORQUES TOO BIG !!!")
                break

            else:
                robot.send_command(command)

        if flag == 2:
            # GET current time
            nIter_time = time.time() - timeZero

            # Check if reached end of traj
            if nIter_time > max_time:
                print("REACHED END OF TRAJ")
                if ctrl_flag == 'Planned':
                    error = np.linalg.norm(full_desired_X[-1, 0:3] - state.ee_state.get_position())
                elif ctrl_flag == 'Replay':
                    error = np.linalg.norm(traj['position'].iat[-1] - state.joint_state.get_positions())
                print("DISTANCE TO TARGET : ", error)

                flag = 3

                # Stop robot
                command.joint_state = state.joint_state
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)

                print("WAIT ...")
                rospy.sleep(1.0)
                continue

            if ctrl_flag == 'Planned':
                ## Feedback state
                feedback_joint_state.set_positions(state.joint_state.get_positions())
                # filter velocities
                dq_filtered = alpha_joint * dq_filtered + (1-alpha_joint) * state.joint_state.get_velocities()
                feedback_joint_state.set_velocities(dq_filtered)

                # cartesian state
                feedback_state.set_position(state.ee_state.get_position())

                # filter velocities
                dx_filtered = alpha * dx_filtered + (1 - alpha) * state.ee_state.get_linear_velocity()
                feedback_state.set_linear_velocity(dx_filtered)

                # orientation
                feedback_state.set_orientation(state.ee_state.get_orientation())
                dq_filtered_ang = alpha_ang * dq_filtered_ang + (1 - alpha_ang) * state.ee_state.get_angular_velocity()
                feedback_state.set_angular_velocity(dq_filtered_ang)

                ## Desired state
                # Get desired state from traj
                X_desired = f_X(nIter_time)
                U_desired = f_U(nIter_time)

                # Set desired angular velocity
                ds_twist = sr.CartesianTwist(ds.evaluate(feedback_state))
                # clamp and set desired angular velocities
                ds_twist.clamp(.25, .25)

                # Get joint positions
                joint_pos = setup_jp(X_desired[0:3],state.joint_state, robot_model)

                desired_joint_state.set_positions(joint_pos)

                desired_state.set_linear_velocity(X_desired[3:6])
                desired_state.set_angular_velocity(ds_twist.data()[3:6])

                # get joint velocities
                joint_vel = np.linalg.lstsq(state.jacobian.data(), desired_state.get_twist(), rcond=None)[0]
                desired_joint_state.set_velocities(joint_vel)

                # compute linear acceleration for data_recording
                if not np.isnan(recorded_state[idx - 1, 0]):
                    acc = (feedback_state.get_linear_velocity() - recorded_state[idx - 1, 46:49]) / (
                                nIter_time - recorded_state[idx - 1, 0])
                    ddq_filtered = beta * ddq_filtered + (1 - beta) * acc

                # SAVE current state
                recorded_state[idx, 0] = nIter_time
                recorded_state[idx, 1:8] = feedback_joint_state.get_positions()
                recorded_state[idx, 8:15] = feedback_joint_state.get_velocities()
                recorded_state[idx, 15:22] = state.joint_state.get_torques()
                recorded_state[idx, 22:29] = desired_joint_state.get_positions()
                recorded_state[idx, 29:36] = desired_joint_state.get_velocities()
                # cartesian state
                recorded_state[idx, 43:46] = feedback_state.get_position()
                recorded_state[idx, 46:49] = feedback_state.get_linear_velocity()
                recorded_state[idx, 49:52] = X_desired[0:3]
                recorded_state[idx, 52:55] = X_desired[3:6]
                # orientation
                recorded_state[idx, 55:59] = feedback_state.get_orientation_coefficients()
                recorded_state[idx, 59:63] = desired_state.get_orientation_coefficients()
                # acceleration
                recorded_state[idx, 63:66] = ddq_filtered
                recorded_state[idx, 66:69] = U_desired
                idx += 1

                # Set command torques
                command_torques = sr.JointTorques(joint_ctrl.compute_command(desired_joint_state, feedback_joint_state))
                # clamp only if last 3 joints exceed joint limits
                if np.any(command_torques.get_torques()[4:6] > 12):
                    print("Command:", command_torques.get_torques())
                    print("TORQUES 5-7 TOO BIG !!! --> CLAMPING")
                    command_torques.clamp(10)

                command.joint_state = state.joint_state
                command.joint_state.set_torques(command_torques.get_torques())

                recorded_state[idx, 36:43] = command.joint_state.get_torques()

                if np.any(command_torques.get_torques() > 30):
                    print("Command:", command_torques.get_torques())
                    print("TORQUES TOO BIG !!!")
                    # break

                robot.send_command(command)

            elif ctrl_flag == 'Replay':
                ## Feedback state
                feedback_joint_state.set_positions(state.joint_state.get_positions())
                # filter velocities
                dq_filtered = alpha_joint * dq_filtered + (1-alpha_joint) * state.joint_state.get_velocities()
                feedback_joint_state.set_velocities(dq_filtered)

                ## Get desired state from traj
                df_desired = find_closest_time(traj, nIter_time)

                desired_joint_state.set_positions(df_desired['position'])
                desired_joint_state.set_velocities(df_desired['velocity'])

                # SAVE current state
                recorded_state[idx, 0] = nIter_time
                recorded_state[idx, 1:8] = state.joint_state.get_positions()
                recorded_state[idx, 8:15] = state.joint_state.get_velocities()
                recorded_state[idx, 15:22] = state.joint_state.get_torques()
                recorded_state[idx, 22:29] = df_desired['position']
                recorded_state[idx, 29:36] = df_desired['velocity']
                recorded_state[idx, 36:43] = df_desired['torques']
                # needed for plots
                recorded_state[idx, 43:69] = np.zeros(26)
                idx += 1

                # Set command torques
                command_torques = sr.JointTorques(joint_ctrl.compute_command(desired_joint_state, feedback_joint_state))
                command.joint_state = state.joint_state
                command.joint_state.set_torques(command_torques.get_torques())

                if np.any(command_torques.get_torques() > 30):
                    print("TORQUES TOO BIG !!!")
                    print("Command:", command.joint_state.get_torques())
                    break

                else:
                    robot.send_command(command)

        if flag == 3:
            # ------------END OF TRAJ-> DO PLOTS ------------------------
            # remove zeros
            recorded_state = recorded_state[~np.isnan(recorded_state).any(axis=1)]

            if ctrl_flag == 'Planned':
                # count number of MPC trajectories and set name for this traj accordingly
                count_prev_traj = sum(len(files) for _, _, files in os.walk(os.path.join(save_dir,correct_label)))
                save_fn = str(count_prev_traj) + "_" + correct_label +"_Planned_eeState"

            elif ctrl_flag == 'Replay':
                # save file, reuse same name
                complete_fn = os.path.split(fpath_list[traj_idx - nbr_planned_traj])
                correct_label = os.path.basename(os.path.normpath(complete_fn[0]))
                fn = os.path.splitext(complete_fn[1])
                save_fn = fn[0] + "_" + correct_label + "_Replay_jointState"

            print("Traj name : ", save_fn)
            # save file
            np.save(os.path.join(save_dir,correct_label,save_fn), recorded_state, allow_pickle=False)

            # option to replay the same trajectory again
            should_replay = input("Do you want to replay this trajectory again ? [y/n] \n")

            if should_replay == 'y' or should_replay == 'Y' or should_replay == 'yes':
                print("Replaying same trajectory, wait for setup... \n")

            else:
                # Get user classification
                classified_as = input("How would you classify this trajectory ? [safe/daring/unsafe] \n")
                user_classification.append(classified_as)
                real_classification.append(correct_label)
                final_fn_list.append(save_fn)
                ctrl_labels.append(ctrl_flag)
                print("Classified as :", classified_as, "\n")

                # in case of crash
                np.savetxt(save_dir + "temp_list_of_labels.txt", np.array(user_classification), fmt='%s')
                np.savetxt(save_dir + "temp_list_of_evaluated_traj.txt", np.array(final_fn_list), fmt='%s')

                print("Setting up next trajectory... \n")

                ## Check there are more traj to play
                if idx_to_incr < (len(idx_list)-1):
                    idx_to_incr += 1
                else:
                    print("PLAYED ALL TRAJECTORIES IN LIST \n")
                    # Save names of replayed trajectories with corresponding labels
                    dict2save = {'Traj name': final_fn_list, 'User Label': user_classification,
                                 'Correct Label': real_classification, 'Type of trajectory': ctrl_labels}
                    df = pd.DataFrame.from_dict(dict2save)
                    df.to_csv(save_dir+"Replayed_demos_and_labels.csv", index=False, header=True)
                    os.remove(save_dir + "temp_list_of_labels.txt")
                    os.remove(save_dir + "temp_list_of_evaluated_traj.txt")

                    # call plot function
                    if do_plots:
                        if ctrl_flag == 'Planned':
                            plot_rec_from_planned_joint(recorded_state, save_fn)
                        elif ctrl_flag == 'Replay':
                            plot_rec_from_replay(traj, recorded_state, save_fn)

                    break

                # call plot function
                if do_plots:
                    if ctrl_flag == 'Planned':
                        plot_rec_from_planned_joint(recorded_state, save_fn)
                    elif ctrl_flag == 'Replay':
                        plot_rec_from_replay(traj, recorded_state, save_fn)

                ## Set up next traj
                traj_idx = idx_list[idx_to_incr]

                # Set up controller flag
                if traj_idx >= nbr_planned_traj:
                    ctrl_flag = 'Replay'
                elif traj_idx < nbr_planned_traj:
                    ctrl_flag = 'Planned'

                ## Set up initial position + grab data
                if ctrl_flag == 'Planned':
                    ## Set up cartesian trajectory + initial position
                    full_desired_X = cart_traj_dict['X'][traj_idx]
                    full_desired_U = cart_traj_dict['U'][traj_idx]
                    full_desired_T = cart_traj_dict['T'][traj_idx]
                    correct_label = cart_traj_dict['Labels'][traj_idx]
                    # append 0 accelerations to U to match size of other variables
                    full_desired_U = np.append(full_desired_U, [[0, 0, 0]], axis=0)
                    initial_joint_pos = setup_jp(full_desired_X[0, 0:3], state.joint_state, robot_model)

                elif ctrl_flag == 'Replay':
                    ## Set up joint trajectory + initial position
                    traj = pd.read_pickle(fpath_list[traj_idx - nbr_planned_traj])
                    initial_joint_pos = traj['position'].at[0]

            # Reset recording list
            recorded_state = np.zeros((15000, 1 + nb_joints * 6 + n_dim * 6 + 8)) * np.nan
            idx = 0
            js_record = np.zeros((15000, 1 + nb_joints * 5)) * np.nan
            js_idx = 0

            # Reset integrator
            integral = np.zeros(7)

            # Reset target + obstacle check
            target = initial_joint_pos
            checked_obstacle = False
            avoiding_obstacle = False

            if in_sim:  # SIMULATOR GAINS
                joint_ctrl.set_parameter_value("stiffness", [10., 5., 4., 2., 2, 2, 2], sr.ParameterType.DOUBLE_ARRAY)
                joint_ctrl.set_parameter_value("damping", [3., 2., 2., 1., .5, .5, .5], sr.ParameterType.DOUBLE_ARRAY)
            else:  # IRL GAINS
                joint_ctrl.set_parameter_value("stiffness", [220, 200, 200, 180, 120, 100, 80], sr.ParameterType.DOUBLE_ARRAY)
                joint_ctrl.set_parameter_value("damping", [15, 15, 15, 13, 9, 8, 6], sr.ParameterType.DOUBLE_ARRAY)

            # Reset flag to start loop again
            input("Ready for next trajectory... Press Enter to start")
            print("Going to start position...")
            start_f0 = time.time()
            flag = 1

        # Publish joint states for recording
        pub.publish(convert_joint_state_msg(state))

        rate.sleep()

def parser():
    # Parse arguments calls functions accordingly
    myparser = argparse.ArgumentParser(description="Plays replays and planned trajectories from user specific CBF planner on Franka Panda robot")

    myparser.add_argument("-u", "--user_number", type=str, default='0', nargs='?', help="The user data to process")
    myparser.add_argument("-n", "--number_of_demos", type=str, default='2',  nargs='?',
                        help="The number of demos to plan/replay for each safety category")
    myparser.add_argument("-r", "--use_replays", type=str, choices=['True', 'False'], default='True',  nargs='?',
                          help="Option only play planned trajectories")
    myparser.add_argument("-p", "--plot", type=str, choices=['True', 'False'], default='False',  nargs='?',
                          help="Option to plot reference vs recorded trajectories")

    args, unknown = myparser.parse_known_args()
    print(args)

    return args.user_number, args.number_of_demos, eval(args.use_replays), eval(args.plot)

### Controller to play replays and planned trajectories from user specific CBF planner
if __name__ == '__main__':

    rospy.init_node("test", anonymous=True)

    # Arguments parser
    user_number, number_of_demos, replays_flag, do_plots = parser()

    # BOOL FOR USING SIMULATOR
    in_sim = False

    print("\n Running User Evaluation Controller for User_"+user_number+"\n")
    print("Total number of demos: "+str(int(number_of_demos)*3 + int(number_of_demos)*3*int(replays_flag))+"\n")

    # Set up directories
    data_dir = "/home/ros/ros_ws/src/learning_safety_margin/data/User_" + user_number + "/"
    save_dir = data_dir + "evaluations/"
    if not os.path.isdir(save_dir):  # create dir if needed
        os.mkdir(save_dir)

    ## Set up Trajectory Planner
    data = pickle.load(open(data_dir + "vel_data_dict.p", "rb"))
    params = data["theta"]
    bias_param = data["bias"]
    # slack_param = data["unsafe_slack"]
    centers = data["rbf_centers"]
    stds = data["rbf_stds"]
    if bias_param is None: bias_param = 0.1

    # SET PARAMETERS FOR MPC CONTROL
    freq = 300
    dt = 0.1  # 1./freq
    n_steps = 50
    nbr_demos = int(number_of_demos)

    # Init Trajectory Generation
    print("Instantiating Traj Planner, should take around %s seconds... \n" % ((n_steps/2)*3))
    instantiate_start = time.time()
    traj_generator = trajGenerator(centers, stds, params, bias_param, dt=dt, n_steps=n_steps, r_gains=0.01, zero_acc_start=True)
    print("FINISH INSTANTIATING TRAJ PLANNER IN %s seconds \n" % (time.time() - instantiate_start))

    print("Planning %s trajectories... \n" % nbr_demos*3)
    plan_start = time.time()
    cbf_traj = traj_generator.generate_all_trajectories(num_demos=nbr_demos)#, init_guess_list=fpath_list)#, plot_debug=show_plots)
    print("FINISH planning trajectories in %s seconds \n" % (time.time() - plan_start))

    ## Make list of trajectories for replaying
    traj_nbr_per_category = int(number_of_demos)
    category_list = ["safe/", "daring/", "unsafe/"]
    fpath_list = []
    for category in category_list:
        ## count nbr of traj in category folder, make a list
        all_fn = glob.glob(data_dir + "csv/" + category + "*.pkl")
        # Set save dir and create folder if needed
        cat_save_dir = save_dir + category
        if not os.path.isdir(cat_save_dir):
            os.mkdir(cat_save_dir)
        # Sample trajectories from category
        fpath_list.append(random.sample(all_fn, traj_nbr_per_category))
    # Reshape list to be 1D, then shuffle
    fpath_list = sum(fpath_list, [])
    random.shuffle(fpath_list)
    # Save name and category of each traj in a txt file
    fn_list = [os.path.join(os.path.basename(os.path.normpath(os.path.split(fn)[0])),
                            os.path.splitext(os.path.split(fn)[1])[0]) for fn in fpath_list]
    print("Traj to be replayed :", fn_list)
    np.savetxt(save_dir + "temp_list_of_replayed_trajectories.txt", np.array(fn_list), fmt='%s')

    # Connect to robot and start control loop
    robot_interface = RobotInterface("*:1701", "*:1702")
    control_loop(cbf_traj, fpath_list, robot_interface, freq, replays_flag, do_plots, in_sim)
