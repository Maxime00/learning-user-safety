#!/usr/bin/env python3

import pandas as pd
import numpy as np
import argparse
# import keyboard
import pickle
import random
import rospy
import glob
import time
import os
import state_representation as sr
from controllers import create_joint_controller, create_cartesian_controller, CONTROLLER_TYPE
from dynamical_systems import create_cartesian_ds, create_joint_ds, DYNAMICAL_SYSTEM_TYPE
from network_interfaces.control_type import ControlType
from network_interfaces.zmq.network import CommandMessage
from learning_safety_margin import RobotInterface
from learning_safety_margin import plot_f0_traj, plot_rec_from_replay, plot_rec_from_planned_joint
from sensor_msgs.msg import JointState
from std_msgs.msg import Header
from learning_safety_margin.obstacle_avoidance import Ellipsoid, doModulation

from learning_safety_margin.cbf_mpc_vel_planner import *
from learning_safety_margin.vel_control_utils import *
from learning_safety_margin.cbf_traj_generator import trajGenerator
from robot_model import Model, InverseKinematicsParameters, QPInverseVelocityParameters
from scipy import interpolate
from pyquaternion import Quaternion

def convert_joint_state_msg(state):
    # Convert sr.joint_state msg to ros JointState msg
    header = Header()
    header.stamp = rospy.get_rostime()
    #header.frame_id = state.get_reference_frame() # is this important ??
    names = state.joint_state.get_names()
    pos = state.joint_state.get_positions()
    vel = state.joint_state.get_velocities()
    effort = state.joint_state.get_torques()
    msg = JointState(header, names, pos, vel, effort)
    return msg

def find_closest_time(df, time):
    dist = (df['time'] - time).abs()
    return df.loc[dist.idxmin()]

def setup_jp(cart_pos, joint_state, robot_model):
    # Set up initial position in Joint State from cartesian state and urdf model
    joint_state_model = sr.JointState('panda', robot_model.get_number_of_joints())
    cart_state = sr.CartesianState("panda_ee", robot_model.get_base_frame())
    cart_state.set_position(cart_pos)
    cart_state.set_orientation(np.array([0., 1., 0., 0.]))
    temp = np.zeros(9)
    temp[0:7] = joint_state.get_positions()
    joint_state_model.set_positions(temp)
    param = InverseKinematicsParameters()
    param.max_number_of_iterations = 15000
    param.tolerance = 0.005
    initial_joint_pos_sr = sr.JointPositions(
        robot_model.inverse_kinematics(sr.CartesianPose(cart_state), sr.JointPositions(joint_state_model), param, "panda_grasptarget"))
    return initial_joint_pos_sr.data()[0:7]

def control_loop(cart_traj_dict, joint_traj_list, robot, freq, traj_type, do_plots, in_sim):

    # create publisher
    pub = rospy.Publisher('/joint_states', JointState, queue_size=10)

    command = CommandMessage()
    command.control_type = [ControlType.EFFORT.value]

    # DS for orientation control
    ds = create_cartesian_ds(DYNAMICAL_SYSTEM_TYPE.POINT_ATTRACTOR)
    ds.set_parameter_value("gain", [0., 0., 0., 5., 5., 5.], sr.ParameterType.DOUBLE_ARRAY)

    # DS to reach initial position
    start_ds = create_cartesian_ds(DYNAMICAL_SYSTEM_TYPE.POINT_ATTRACTOR)
    start_ds.set_parameter_value("gain", [10., 10., 10., 10., 10., 10.], sr.ParameterType.DOUBLE_ARRAY)

    # ADD OBSTACLE
    obstacle = Ellipsoid([0.55, 0.0, 0.035], [.10, .12, .12 ], 0.5) # x=0.55 when table obstacle against table

    # SET UP TORQUE controller - reach start position
    nb_joints = 7
    joint_ctrl = create_joint_controller(CONTROLLER_TYPE.IMPEDANCE, nb_joints)

    if in_sim:  # SIMULATOR GAINS
        joint_ctrl.set_parameter_value("stiffness", [10., 5., 4., 2., 2, 2, 2], sr.ParameterType.DOUBLE_ARRAY)
        joint_ctrl.set_parameter_value("damping", [3., 2., 2., 1., .5, .5, .5], sr.ParameterType.DOUBLE_ARRAY)
        stiffness = np.array([10., 5., 4., 2., 2, 2, 2])
        damping = np.array([3., 2., 2., 1., .5, .5, .5])
    else:  # IRL GAINS
        # Getting to start position
        # stiffness = np.array([220, 220, 200, 180, 110, 100, 40]) * 0.3
        # damping = np.array([18, 17, 15.5, 13, 6, 5, 1.2]) * 2.0  ## Works well, bit too high in velocity for reaching start
        stiffness_start = np.array([220, 220, 220, 180, 120, 100, 50]) * 0.5
        damping_start = np.array([15, 15, 15, 13, 8, 7, 4]) * 0.8
        stiffness = np.array([220, 220, 220, 180, 100, 80, 50]) * 0.3
        damping = np.array([18, 17, 15, 14, 8, 7, 4]) * 1.8
        # joint_ctrl.set_parameter_value("stiffness", [220, 200, 200, 180, 120, 100, 80], sr.ParameterType.DOUBLE_ARRAY)
        # joint_ctrl.set_parameter_value("damping", [15, 15, 15, 13, 9, 8, 6], sr.ParameterType.DOUBLE_ARRAY)
        joint_ctrl.set_parameter_value("stiffness", stiffness_start.tolist(), sr.ParameterType.DOUBLE_ARRAY)
        joint_ctrl.set_parameter_value("damping", damping_start.tolist(), sr.ParameterType.DOUBLE_ARRAY)
        # Trajectory following
    # Import panda urdf
    urdf_path = "/home/ros/ros_ws/src/learning_safety_margin/urdf/panda_arm_hand.urdf"
    robot_model = Model('panda', urdf_path)

    # Set up state variables
    desired_state = sr.CartesianState('panda_ee', 'panda_base')
    feedback_state = sr.CartesianState('panda_ee', 'panda_base')
    desired_joint_state = sr.JointState("franka", nb_joints)
    feedback_joint_state = sr.JointState("franka", nb_joints)

    flag = 1
    print_freq = freq # *X (prints every X seconds)

    # Reaching start logic
    reached_start = False
    reaching_jp = False

    # Integrator Variables
    start_joint_control_m = 0.10 #m cartesian distance to start controlling joint position to reach start jp
    start_integrator_rad = 0.3 #rad # ideal 0.25 but doesn't reach that sometimes
    start_integrator_m = 0.12 #m #threshold for itnegrator in Planned
    init_pos_err_threshold_cart = 0.01 # in m
    init_pos_err_threshold_joint = 0.02 # in rad
    integrator_reset_threshold = 0.05#0.10 # in rad #0.05
    time_period = 1. / freq
    integral = np.zeros(nb_joints)
    inte_torques = np.zeros(nb_joints)
    dq_filtered = np.zeros(nb_joints)
    null_command = np.zeros(nb_joints)

    # filter parameters
    n_dim = 3  # cartesian space[x,y,z]]
    alpha_joint = 0.4
    alpha = 0.8
    alpha_ang = 0.8
    dx_filtered = np.zeros(n_dim)
    dq_filtered_ang = np.zeros(n_dim)
    beta = 0.95
    ddq_filtered = np.zeros(n_dim)

    # Set up data recording
    recorded_state = np.zeros((15000, 1 + nb_joints * 6 + n_dim * 6 + 8)) * np.nan
    idx = 0

    js_record = np.zeros((15000, 1 + nb_joints * 5)) * np.nan
    js_idx = 0

    user_classification = []
    real_classification = []
    final_fn_list = []
    ctrl_labels = []

    # wait a bit to receive correct state
    checked_first_state = False
    checked_obstacle = False

    home_position = np.array([0.0, 0.0, 0.0,  -1.6478, 0.0, 1.6295, 0.785])
    null_position = np.array([0.0, 0.1479, 0.0, -2.2249, 0.0, 2.3723, 0.7])  ## home pos np.array([0.0, 0.0, 0.0,  -1.6478, 0.0, 1.6295, 0.785])
    if in_sim: null_gain = 0.0
    else: null_gain = 2.0

    # Successive traj follow - make random idx list to shuffle planned trajectories
    nbr_planned_traj = len(cart_traj_dict['Labels'])
    # print("CHECK NUMBER OF PLANNED TRAJ :", nbr_planned_traj)

    # Whether to use replays, planned trajectories or both (default is both)
    if traj_type == 'both':
        idx_list = list(range(0, nbr_planned_traj+len(joint_traj_list)))
    elif traj_type == 'planned':
        idx_list = list(range(0, nbr_planned_traj))
    elif traj_type == 'replay':
        idx_list = list(range(nbr_planned_traj, nbr_planned_traj+len(joint_traj_list)))

    random.shuffle(idx_list)
    idx_to_incr = 0

    ### Set up first trajectory
    traj_idx = idx_list[idx_to_incr]

    # Set up controller flag
    if traj_idx >= nbr_planned_traj:
        ctrl_flag = 'Replay'
    elif traj_idx < nbr_planned_traj:
        ctrl_flag = 'Planned'

    ## Set up initial position + grab data
    if ctrl_flag == 'Planned':
        ## Set up cartesian trajectory + initial position
        full_desired_X = cart_traj_dict['X'][traj_idx]
        full_desired_U = cart_traj_dict['U'][traj_idx]
        full_desired_T = cart_traj_dict['T'][traj_idx]
        correct_label = cart_traj_dict['Labels'][traj_idx]
        # append 0 accelerations to U to match size of other variables
        full_desired_U = np.append(full_desired_U, [[0, 0, 0]], axis=0)
        initial_joint_pos = setup_jp(full_desired_X[0, 0:3], sr.JointPositions(robot_model.get_robot_name(),home_position), robot_model)
        initial_cart_pose = sr.CartesianPose('panda_ee', full_desired_X[0, 0:3],  np.array([0., 1., 0., 0.]), 'panda_base')
        # Set integrator gains
        inte_gain = np.array([65, 55, 50, 45, 40, 30, 20]) * 0.5

    elif ctrl_flag == 'Replay':
        ## Set up joint trajectory + initial position
        traj = pd.read_pickle(fpath_list[traj_idx - nbr_planned_traj])
        initial_joint_pos = traj['position'].at[0]
        joint_state_model = sr.JointState('panda', robot_model.get_number_of_joints())
        temp = np.zeros(9)
        temp[0:7] = initial_joint_pos
        joint_state_model.set_positions(temp)
        temp = sr.CartesianPose(robot_model.forward_kinematics(sr.JointPositions(joint_state_model), "panda_grasptarget"))
        # Hard code orientation - known due to fixed orientation in data recording
        initial_cart_pose = sr.CartesianPose('panda_ee', temp.get_position(), np.array([0., 1., 0., 0.]), 'panda_base')
        # Set integrator gains
        inte_gain = np.array([65, 55, 50, 45, 40, 30, 20]) * 1.0

    if in_sim: inte_gain = [3, 2.5, 2, 1.8, 1.5, 1.3, 1]  # SIM GAINS
    start_ds.set_parameter_value("attractor", initial_cart_pose, sr.ParameterType.STATE, sr.StateType.CARTESIAN_POSE)

    print_count = 0
    target = initial_joint_pos

    ### CONTROL LOOP
    rate = rospy.Rate(freq)
    while not rospy.is_shutdown():

        state = robot.get_state()


        if not state:
            continue

        # Wait a bit so state is correct
        elif state and not checked_first_state:
            # print("EEF position: ", state.ee_state.get_position())
            rospy.sleep(0.1)
            checked_first_state = True
            input("Waiting for user input... Press Enter to start")
            print("Going to home position...")
            print("panda base ref frame :", state.ee_state.get_reference_frame())
            start_f0 = time.time()
            continue

        if flag == 1:
            #----------- GO TO START POSITION ---------------------

            error_cart = abs(initial_cart_pose.get_position() - state.ee_state.get_position())
            error_joint = abs(initial_joint_pos - state.joint_state.get_positions())
            time_f0 = time.time() - start_f0

            # Check for time, move on if takes too long
            if time_f0 > 12.0 :
                # Stop robot
                command.joint_state = state.joint_state
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)
                rospy.sleep(0.2)
                robot.send_command(command)

                print("EXCEEDED ALLOTED TIME TO REACH START POSITION")
                print("CARTESIAN ERROR :", error_cart)
                print("JP ERROR:", error_joint)
                moving_on = input("Attempt to play trajectory anyway?\n")
                if moving_on == 'y' or moving_on=='Y' or moving_on=='yes':
                    reached_start = True
                else:
                    flag = 3
                    continue

            if ctrl_flag == 'Planned' and np.all(error_cart < init_pos_err_threshold_cart):
                reached_start = True
            if ctrl_flag == 'Replay' and np.all(error_joint < init_pos_err_threshold_joint):
                reached_start = True

            if reached_start:
                print("Error :", error_cart)
                print("Reached start position in %s seconds" % time_f0)
                flag = 2

                # Stop robot
                command.joint_state = state.joint_state
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)
                rospy.sleep(0.2)
                robot.send_command(command)

                ## plot f0 traj
                js_record = js_record[~np.isnan(js_record).any(axis=1)]  # remove zeros
                if do_plots:
                    plot_f0_traj(js_record)

                ## Setup trajectory following
                if ctrl_flag == 'Planned':
                    # Create interpolation functions
                    f_X = interpolate.interp1d(full_desired_T, full_desired_X, axis=0)
                    f_U = interpolate.interp1d(full_desired_T, full_desired_U, axis=0)

                    # set DS target for orientation control
                    target_ds = sr.CartesianPose(state.ee_state.get_name(),  full_desired_X[-1, 0:3],  np.array([0., 1., 0., 0.]),
                                                state.ee_state.get_reference_frame())
                    ds.set_parameter_value("attractor", target_ds, sr.ParameterType.STATE, sr.StateType.CARTESIAN_POSE)
                    # Set up for end of traj follow
                    max_time = full_desired_T[-1]

                    joint_ctrl.set_parameter_value("stiffness", stiffness.tolist(), sr.ParameterType.DOUBLE_ARRAY)
                    joint_ctrl.set_parameter_value("damping", damping.tolist(), sr.ParameterType.DOUBLE_ARRAY)

                    print("Ready to play planned trajectory labeled #%s of %s [Progress: %s/%s] \n" % (traj_idx + 1, nbr_planned_traj, idx_to_incr+1, len(idx_list)))

                elif ctrl_flag == 'Replay':
                    # Set up for end of traj follow
                    max_time = traj['time'].iat[-1]
                    print("Ready to play replay trajectory labeled #%s of %s [Progress: %s/%s] \n" % (traj_idx-nbr_planned_traj + 1, len(fpath_list), idx_to_incr+1, len(idx_list)))

                input("Waiting for user input... Press Enter to play trajectory \n ")
                rospy.sleep(0.2)
                timeZero = time.time()
                continue

            # Pass here once to start joint position control
            if np.all(error_cart < start_joint_control_m) and ctrl_flag == 'Replay' and reaching_jp is False:
                #Activate jp control and continue
                reaching_jp = True
                command.joint_state = state.joint_state
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)
                rospy.sleep(0.1)    # To avoid going through the target and stopping too late
                robot.send_command(command)
                print("SWITCHED TO JOINT CONTROL !!  \n  ")
                continue

            # Adding integrator when getting close to target
            if (np.all(error_joint < start_integrator_rad) and reaching_jp) or (np.all(error_cart < start_integrator_m) and ctrl_flag == 'Planned'):
                integral += (initial_joint_pos - state.joint_state.get_positions()) * time_period

                # set different gains for each controller
                if ctrl_flag == 'Replay':
                    # reset integral if overhsoot
                    if np.any(abs(integral) > integrator_reset_threshold):
                        print("RESETTING INTEGRATOR ")
                        integral *= 0
                elif ctrl_flag == 'Planned':
                    if np.any(abs(integral) > 2*integrator_reset_threshold):
                        print("RESETTING INTEGRATOR ")
                        integral *= 0
                inte_torques = inte_gain * integral
                if print_count % print_freq == 0:
                    print("USING INTEGRATOR ")

            # Set feedback state
            feedback_joint_state.set_positions(state.joint_state.get_positions())
            dq_filtered = alpha_joint * dq_filtered + (1-alpha_joint) * state.joint_state.get_velocities()
            feedback_joint_state.set_velocities(dq_filtered)

            # Use DS with modulation to avoidobstacle, when close to replay start position, switch to joint control
            if reaching_jp:
                # Set velocity and position error
                velo_d = initial_joint_pos - state.joint_state.get_positions()
                # Norm velocities
                velo_d = velo_d / np.linalg.norm(velo_d) if np.linalg.norm(velo_d) > 1.0 else velo_d
            else:
                # Get twist from DS
                ds_twist = sr.CartesianTwist(start_ds.evaluate(state.ee_state))
                ds_twist.clamp(.25, .5)
                ## ADD MODULATION
                modulated_lin_vel = doModulation(obstacle, state.ee_state.get_position(), ds_twist.get_linear_velocity())
                # mod_dif = ds_twist.get_linear_velocity()-modulated_lin_vel
                ds_twist.set_linear_velocity(modulated_lin_vel)

                # clamp and set desired angular velocities
                ds_twist.clamp(.25, .5)
                velo_d = np.linalg.lstsq(state.jacobian.data(), ds_twist.data(), rcond=None)[0]
                ## NULL SPACE
                # Only remove when aiming for replay initial joint position
                # Calculate jacobian pseudo inverse
                jacobian = np.array(state.jacobian.data())
                jacobian_damped_inverse = np.dot(jacobian.T, np.linalg.inv((np.dot(jacobian, jacobian.T) + 0.01 * np.eye(6))))
                # calculate null space matrix and error
                null_space_matrix = np.eye(nb_joints) - jacobian_damped_inverse.dot(jacobian)
                null_error = null_position - state.joint_state.get_positions()
                # get null space command
                null_command = null_gain * null_space_matrix.dot(null_error)

            desired_joint_state.set_velocities(velo_d)

            pos_d = feedback_joint_state.get_positions() + time_period * velo_d
            desired_joint_state.set_positions(pos_d)

            ## NULL SPACE
            # Calculate jacobian pseudo inverse
            jacobian = np.array(state.jacobian.data())
            jacobian_damped_inverse = np.dot(jacobian.T, np.linalg.inv((np.dot(jacobian, jacobian.T) + 0.01 * np.eye(6))))
            # calculate null space matrix and error
            null_space_matrix = np.eye(nb_joints) - jacobian_damped_inverse.dot(jacobian)
            null_error = null_position - state.joint_state.get_positions()
            # get null space command
            null_command = null_gain * null_space_matrix.dot(null_error)

            # Set command
            command_torques = sr.JointTorques(joint_ctrl.compute_command(desired_joint_state, feedback_joint_state))
            command.joint_state = state.joint_state
            command.joint_state.set_torques(command_torques.get_torques() + inte_torques + null_command)

            # record data for plots
            if js_idx < len(js_record[:,0]):
                js_record[js_idx, 0] = time_f0
                js_record[js_idx, 1:8] = state.joint_state.get_positions()
                js_record[js_idx, 8:15] = state.joint_state.get_velocities()
                js_record[js_idx, 15:22] = initial_joint_pos
                js_record[js_idx, 22:29] = integral
                js_record[js_idx, 29:36] = inte_torques
                js_idx += 1

            # debug print
            if print_count % print_freq == 0:
                print("CTRL FLAG:", ctrl_flag)
                # print("MODULATION DIFFERENCE:", mod_dif)
                # print("Error :", error_joint)
                # print("Command:", command_torques.get_torques())
                print("TARGET:", initial_cart_pose.get_position())
                print("CURR POS:", state.ee_state.get_position())
                print("ERROR CART:", error_cart)
                print("ERROR:", error_joint)
                # if ctrl_flag == 'Planned': print("DS twist post clamping : ", ds_twist)
            print_count += 1

            # print("Command:", command.joint_state.get_torques())
            if np.any(command_torques.get_torques() > 30):
                print("Command:", command_torques.get_torques())
                print("TORQUES TOO BIG !!!")
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)
                rospy.sleep(0.2)
                robot.send_command(command)
                # break
            else:
                robot.send_command(command)

        if flag == 2:
            # GET current time
            nIter_time = time.time() - timeZero

            # Check if reached end of traj
            if nIter_time > max_time:
                print("REACHED END OF TRAJ")
                if ctrl_flag == 'Planned':
                    error = np.linalg.norm(full_desired_X[-1, 0:3] - state.ee_state.get_position())
                elif ctrl_flag == 'Replay':
                    error = np.linalg.norm(traj['position'].iat[-1] - state.joint_state.get_positions())
                print("DISTANCE TO TARGET : ", error)

                flag = 3

                # Stop robot
                command.joint_state = state.joint_state
                command.joint_state.set_torques(np.zeros(7))
                robot.send_command(command)
                rospy.sleep(0.2)
                robot.send_command(command)
                print("WAIT ...")
                rospy.sleep(0.2)
                continue

            if ctrl_flag == 'Planned':
                ## Feedback state
                feedback_joint_state.set_positions(state.joint_state.get_positions())
                # filter velocities
                dq_filtered = alpha_joint * dq_filtered + (1-alpha_joint) * state.joint_state.get_velocities()
                feedback_joint_state.set_velocities(dq_filtered)

                # cartesian state
                feedback_state.set_position(state.ee_state.get_position())

                # filter velocities
                dx_filtered = alpha * dx_filtered + (1 - alpha) * state.ee_state.get_linear_velocity()
                feedback_state.set_linear_velocity(dx_filtered)

                # orientation
                feedback_state.set_orientation(state.ee_state.get_orientation())
                dq_filtered_ang = alpha_ang * dq_filtered_ang + (1 - alpha_ang) * state.ee_state.get_angular_velocity()
                feedback_state.set_angular_velocity(dq_filtered_ang)

                ## Desired state
                # Get desired state from traj
                X_desired = f_X(nIter_time)
                U_desired = f_U(nIter_time)

                # Set desired angular velocity
                ds_twist = sr.CartesianTwist(ds.evaluate(feedback_state))
                # clamp and set desired angular velocities
                ds_twist.clamp(.25, .5)

                # Get joint positions
                joint_pos = setup_jp(X_desired[0:3],state.joint_state, robot_model)

                desired_joint_state.set_positions(joint_pos)

                desired_state.set_linear_velocity(X_desired[3:6])
                desired_state.set_angular_velocity(ds_twist.data()[3:6])

                # Get joint velocities
                joint_vel = np.linalg.lstsq(state.jacobian.data(), desired_state.get_twist(), rcond=None)[0]
                desired_joint_state.set_velocities(joint_vel)

                # compute linear acceleration for data_recording
                if not np.isnan(recorded_state[idx - 1, 0]):
                    acc = (feedback_state.get_linear_velocity() - recorded_state[idx - 1, 46:49]) / (
                                nIter_time - recorded_state[idx - 1, 0])
                    ddq_filtered = beta * ddq_filtered + (1 - beta) * acc

                # SAVE current state
                recorded_state[idx, 0] = nIter_time
                recorded_state[idx, 1:8] = feedback_joint_state.get_positions()
                recorded_state[idx, 8:15] = feedback_joint_state.get_velocities()
                recorded_state[idx, 15:22] = state.joint_state.get_torques()
                recorded_state[idx, 22:29] = desired_joint_state.get_positions()
                recorded_state[idx, 29:36] = desired_joint_state.get_velocities()
                # cartesian state
                recorded_state[idx, 43:46] = feedback_state.get_position()
                recorded_state[idx, 46:49] = feedback_state.get_linear_velocity()
                recorded_state[idx, 49:52] = X_desired[0:3]
                recorded_state[idx, 52:55] = X_desired[3:6]
                # orientation
                recorded_state[idx, 55:59] = feedback_state.get_orientation_coefficients()
                recorded_state[idx, 59:63] = desired_state.get_orientation_coefficients()
                # acceleration
                recorded_state[idx, 63:66] = ddq_filtered
                recorded_state[idx, 66:69] = U_desired
                idx += 1

                # Set command torques
                command_torques = sr.JointTorques(joint_ctrl.compute_command(desired_joint_state, feedback_joint_state))
                # clamp only if last 3 joints exceed joint limits
                if np.any(command_torques.get_torques()[4:6] > 12):
                    print("Command:", command_torques.get_torques())
                    print("TORQUES 5-7 TOO BIG !!! --> CLAMPING")
                    command_torques.clamp(10)

                command.joint_state = state.joint_state
                command.joint_state.set_torques(command_torques.get_torques())

                recorded_state[idx, 36:43] = command.joint_state.get_torques()

                if np.any(command_torques.get_torques() > 30):
                    print("Command:", command_torques.get_torques())
                    print("TORQUES TOO BIG !!!")
                    command.joint_state.set_torques(np.zeros(7))
                    robot.send_command(command)
                    # break
                else:
                    robot.send_command(command)

            elif ctrl_flag == 'Replay':
                ## Feedback state
                feedback_joint_state.set_positions(state.joint_state.get_positions())
                # filter velocities
                dq_filtered = alpha_joint * dq_filtered + (1-alpha_joint) * state.joint_state.get_velocities()
                feedback_joint_state.set_velocities(dq_filtered)

                ## Get desired state from traj
                df_desired = find_closest_time(traj, nIter_time)

                desired_joint_state.set_positions(df_desired['position'])
                desired_joint_state.set_velocities(df_desired['velocity'])

                # SAVE current state
                recorded_state[idx, 0] = nIter_time
                recorded_state[idx, 1:8] = state.joint_state.get_positions()
                recorded_state[idx, 8:15] = state.joint_state.get_velocities()
                recorded_state[idx, 15:22] = state.joint_state.get_torques()
                recorded_state[idx, 22:29] = df_desired['position']
                recorded_state[idx, 29:36] = df_desired['velocity']
                recorded_state[idx, 36:43] = df_desired['torques']
                # needed for plots
                recorded_state[idx, 43:69] = np.zeros(26)
                idx += 1

                # Set command torques
                command_torques = sr.JointTorques(joint_ctrl.compute_command(desired_joint_state, feedback_joint_state))
                command.joint_state = state.joint_state
                command.joint_state.set_torques(command_torques.get_torques())

                if np.any(command_torques.get_torques() > 30):
                    print("TORQUES TOO BIG !!!")
                    print("Command:", command.joint_state.get_torques())
                    command.joint_state.set_torques(np.zeros(7))
                    robot.send_command(command)
                    # break

                else:
                    robot.send_command(command)

        if flag == 3:
            # ------------END OF TRAJ-> DO PLOTS ------------------------
            # remove zeros
            recorded_state = recorded_state[~np.isnan(recorded_state).any(axis=1)]

            if ctrl_flag == 'Planned':
                # count number of MPC trajectories and set name for this traj accordingly
                count_prev_traj = sum(len(files) for _, _, files in os.walk(os.path.join(save_dir,correct_label)))
                save_fn = str(count_prev_traj) + "_" + correct_label +"_Planned"

            elif ctrl_flag == 'Replay':
                # save file, reuse same name
                complete_fn = os.path.split(fpath_list[traj_idx - nbr_planned_traj])
                correct_label = os.path.basename(os.path.normpath(complete_fn[0]))
                fn = os.path.splitext(complete_fn[1])
                save_fn = fn[0] + "_" + correct_label + "_Replay"

            print("Traj name : ", save_fn)
            # save file
            np.save(os.path.join(save_dir,correct_label,save_fn), recorded_state, allow_pickle=False)

            # option to replay the same trajectory again
            should_replay = input("Do you want to replay this trajectory again ? [y/n] \n")

            if should_replay == 'y' or should_replay == 'Y' or should_replay == 'yes':
                print("Replaying same trajectory, wait for setup... \n")

            else:
                # Get user classification
                classified_as = input("How would you classify this trajectory ? [safe/daring/unsafe] \n")
                user_classification.append(classified_as)
                real_classification.append(correct_label)
                final_fn_list.append(save_fn)
                ctrl_labels.append(ctrl_flag)
                print("Classified as :", classified_as, "\n")

                # in case of crash
                np.savetxt(save_dir + "temp_list_of_labels.txt", np.array(user_classification), fmt='%s')
                np.savetxt(save_dir + "temp_list_of_evaluated_traj.txt", np.array(final_fn_list), fmt='%s')

                print("Setting up next trajectory... \n")

                ## Check there are more traj to play
                if idx_to_incr < (len(idx_list)-1):
                    idx_to_incr += 1
                else:
                    print("PLAYED ALL TRAJECTORIES IN LIST \n")
                    # Save names of replayed trajectories with corresponding labels
                    dict2save = {'Traj name': final_fn_list, 'User Label': user_classification,
                                 'Correct Label': real_classification, 'Type of trajectory': ctrl_labels}
                    df = pd.DataFrame.from_dict(dict2save)
                    df.to_csv(save_dir+"Replayed_demos_and_labels.csv", index=False, header=True)
                    os.remove(save_dir + "temp_list_of_labels.txt")
                    os.remove(save_dir + "temp_list_of_evaluated_traj.txt")

                    # call plot function
                    if do_plots:
                        if ctrl_flag == 'Planned':
                            plot_rec_from_planned_joint(recorded_state, save_fn)
                        elif ctrl_flag == 'Replay':
                            plot_rec_from_replay(traj, recorded_state, save_fn)

                    break

                # Call plot function
                if do_plots:
                    if ctrl_flag == 'Planned':
                        plot_rec_from_planned_joint(recorded_state, save_fn)
                    elif ctrl_flag == 'Replay':
                        plot_rec_from_replay(traj, recorded_state, save_fn)

                ## Set up next traj
                traj_idx = idx_list[idx_to_incr]

                # Set up controller flag
                if traj_idx >= nbr_planned_traj:
                    ctrl_flag = 'Replay'
                elif traj_idx < nbr_planned_traj:
                    ctrl_flag = 'Planned'

                ## Set up initial position + grab data
                if ctrl_flag == 'Planned':
                    ## Set up cartesian trajectory + initial position
                    full_desired_X = cart_traj_dict['X'][traj_idx]
                    full_desired_U = cart_traj_dict['U'][traj_idx]
                    full_desired_T = cart_traj_dict['T'][traj_idx]
                    correct_label = cart_traj_dict['Labels'][traj_idx]
                    # append 0 accelerations to U to match size of other variables
                    full_desired_U = np.append(full_desired_U, [[0, 0, 0]], axis=0)
                    initial_joint_pos = setup_jp(full_desired_X[0, 0:3],
                                                 sr.JointPositions(robot_model.get_robot_name(), home_position),
                                                 robot_model)
                    initial_cart_pose = sr.CartesianPose('panda_ee', full_desired_X[0, 0:3],
                                                         np.array([0., 1., 0., 0.]), 'panda_base')
                    # Set integrator gains
                    inte_gain = np.array([65, 55, 50, 45, 40, 30, 20]) * 0.5

                elif ctrl_flag == 'Replay':
                    ## Set up joint trajectory + initial position
                    traj = pd.read_pickle(fpath_list[traj_idx - nbr_planned_traj])
                    initial_joint_pos = traj['position'].at[0]
                    joint_state_model = sr.JointState('panda', robot_model.get_number_of_joints())
                    temp = np.zeros(9)
                    temp[0:7] = initial_joint_pos
                    joint_state_model.set_positions(temp)
                    temp = sr.CartesianPose(
                        robot_model.forward_kinematics(sr.JointPositions(joint_state_model), "panda_grasptarget"))
                    # Hard code orientation - known due to fixed orientation in data recording
                    initial_cart_pose = sr.CartesianPose('panda_ee', temp.get_position(),
                                                         np.array([0., 1., 0., 0.]), 'panda_base')
                    # Set integrator gains
                    inte_gain = np.array([65, 55, 50, 45, 40, 30, 20]) * 1.0

            start_ds.set_parameter_value("attractor", initial_cart_pose, sr.ParameterType.STATE,
                                         sr.StateType.CARTESIAN_POSE)

            # Reset obstacle avoidance
            reached_start = False
            reaching_jp = False

            # Reset recording list
            recorded_state = np.zeros((15000, 1 + nb_joints * 6 + n_dim * 6 + 8)) * np.nan
            idx = 0
            js_record = np.zeros((15000, 1 + nb_joints * 5)) * np.nan
            js_idx = 0

            # Reset integrator
            integral = np.zeros(nb_joints)
            inte_torques = np.zeros(nb_joints)
            dq_filtered = np.zeros(nb_joints)
            null_command = np.zeros(nb_joints)

            # Reset target + obstacle check
            target = initial_joint_pos
            checked_obstacle = False
            avoiding_obstacle = False

            if in_sim:  # SIMULATOR GAINS
                joint_ctrl.set_parameter_value("stiffness", [10., 5., 4., 2., 2, 2, 2], sr.ParameterType.DOUBLE_ARRAY)
                joint_ctrl.set_parameter_value("damping", [3., 2., 2., 1., .5, .5, .5], sr.ParameterType.DOUBLE_ARRAY)
            else:  # IRL GAINS
                # joint_ctrl.set_parameter_value("stiffness", [220, 200, 200, 180, 120, 100, 80], sr.ParameterType.DOUBLE_ARRAY)
                # joint_ctrl.set_parameter_value("damping", [15, 15, 15, 13, 9, 8, 6], sr.ParameterType.DOUBLE_ARRAY)
                joint_ctrl.set_parameter_value("stiffness", stiffness_start.tolist(), sr.ParameterType.DOUBLE_ARRAY)
                joint_ctrl.set_parameter_value("damping", damping_start.tolist(), sr.ParameterType.DOUBLE_ARRAY)

            # Reset flag to start loop again
            input("Ready for next trajectory... Press Enter to start")
            print("Going to start position...")
            start_f0 = time.time()
            flag = 1

        # Publish joint states for recording
        pub.publish(convert_joint_state_msg(state))

        rate.sleep()

def traj_selector(traj_dict, planned_traj_to_keep):
    df = pd.DataFrame.from_dict(traj_dict)
    new_df = df.iloc[planned_traj_to_keep]
    return new_df

def parser():
    # Parse arguments calls functions accordingly
    myparser = argparse.ArgumentParser(description="Plays replays and planned trajectories from user specific CBF planner on Franka Panda robot")

    myparser.add_argument("-u", "--user_number", type=str, default='0', nargs='?', help="The user data to process")
    myparser.add_argument("-n", "--number_of_demos", type=str, default='2',  nargs='?',
                        help="The number of demos to plan/replay for each safety category")
    myparser.add_argument("-t", "--trajectory_type", type=str, choices=['planned', 'replay', 'both'], default='both',  nargs='?',
                          help="Option only select type of trajectories")
    myparser.add_argument("-p", "--plot", type=str, choices=['True', 'False'], default='False',  nargs='?',
                          help="Option to plot reference vs recorded trajectories")
    myparser.add_argument("-g", "--generate_trajectories", type=str, choices=['True', 'False'], default='False',  nargs='?',
                          help="Option to generate planned traj or used previous dict trajectories")

    args, unknown = myparser.parse_known_args()

    print(args)

    return args.user_number, args.number_of_demos, args.trajectory_type, eval(args.plot), eval(args.generate_trajectories)

### Controller to play replays and planned trajectories from user specific CBF planner
if __name__ == '__main__':

    rospy.init_node("test", anonymous=True)

    # Arguments parser
    user_number, number_of_demos, traj_type, do_plots, gen_traj = parser()

    # BOOL FOR USING SIMULATOR
    in_sim = False
    if in_sim:
        print("USING SIMULATION PARAMETERS")

    # SET PARAMETERS FOR MPC CONTROL
    freq = 300
    dt = 0.1  # 1./freq
    n_steps = 50
    nbr_demos = int(number_of_demos)

    # PRINT INFO
    print("\n Running User Evaluation Controller for User_"+user_number+"\n")
    if traj_type == 'planned': total_nbr_demos = nbr_demos*2
    elif traj_type == 'replay': total_nbr_demos = nbr_demos * 3
    elif traj_type == 'both': total_nbr_demos = nbr_demos * 5
    print("Total number of demos: "+str(total_nbr_demos)+"\n")

    # Set up directories
    data_dir = "/home/ros/ros_ws/src/learning_safety_margin/data/User_" + user_number + "/"
    save_dir = data_dir + "evaluations/"
    if not os.path.isdir(save_dir):  # create dir if needed
        os.mkdir(save_dir)

    ## Set up Trajectory Planner
    data = pickle.load(open(data_dir + "vel_data_dict.p", "rb"))
    params = data["theta"]
    bias_param = data["bias"]
    # slack_param = data["unsafe_slack"]
    centers = data["rbf_centers"]
    stds = data["rbf_stds"]
    if bias_param is None: bias_param = 0.1

    ## DEBUG PRINTs
    print(params.shape)
    print(centers.shape)

    # Generate traj if required, used previous generated ones from saved dictionary otherwise
    if gen_traj:
        # Init Trajectory Generation
        print("Instantiating Traj Planner, should take around %s seconds... \n" % ((n_steps/2)*3))
        instantiate_start = time.time()
        traj_generator = trajGenerator(centers, stds, params, bias_param, dt=dt, n_steps=n_steps, r_gains=0.01, zero_acc_start=True)
        print("FINISH INSTANTIATING TRAJ PLANNER IN %s seconds \n" % (time.time() - instantiate_start))

        print("Planning %s trajectories... \n" %(nbr_demos*2))
        plan_start = time.time()
        cbf_traj = traj_generator.generate_all_trajectories(num_demos=nbr_demos)#, init_guess_list=fpath_list)#, plot_debug=show_plots)
        with open(data_dir+str(nbr_demos*2)+'_planned_trajectories.pkl', 'wb') as f:
            pickle.dump(cbf_traj, f)
        print("FINISH planning trajectories in %s seconds \n" % (time.time() - plan_start))
    else:
        print("LOADING PLANNED TRAJECTORIES FROM SAVED FILE")
        with open(data_dir+'30_planned_trajectories.pkl', 'rb') as f:
            cbf_traj = pickle.load(f)

    ## Make list of trajectories for replaying
    traj_nbr_per_category = int(number_of_demos)
    category_list = ["safe/", "daring/","unsafe/"] ##
    fpath_list = []
    for category in category_list:
        ## count nbr of traj in category folder, make a list
        all_fn = glob.glob(data_dir + "csv/" + category + "*.pkl")
        # Set save dir and create folder if needed
        cat_save_dir = save_dir + category
        if not os.path.isdir(cat_save_dir):
            os.mkdir(cat_save_dir)
        # Sample trajectories from category
        fpath_list.append(random.sample(all_fn, traj_nbr_per_category))
    # Reshape list to be 1D, then shuffle
    fpath_list = sum(fpath_list, [])
    random.shuffle(fpath_list)

    # CHOOSE TRAJECTORIES MANUALLY
    # fpath_list[0] = data_dir + "csv/" + "unsafe/13_jointState.pkl"
    # cbf_traj = traj_selector(cbf_traj, [2, 7, 8, 5, 11, 12])

    # Save name and category of each traj in a txt file
    fn_list = [os.path.join(os.path.basename(os.path.normpath(os.path.split(fn)[0])),
                            os.path.splitext(os.path.split(fn)[1])[0]) for fn in fpath_list]

    print("Traj to be replayed :", fn_list)
    np.savetxt(save_dir + "temp_list_of_replayed_trajectories.txt", np.array(fn_list), fmt='%s')

    # Connect to robot and start control loop
    robot_interface = RobotInterface("*:1701", "*:1702")
    control_loop(cbf_traj, fpath_list, robot_interface, freq, traj_type, do_plots, in_sim)
